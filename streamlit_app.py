import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import os
import sys

# Set page configuration
st.set_page_config(
    page_title="Air Quality Data Analysis",
    page_icon="üåç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Define paths
current_dir = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(current_dir, "AirQualityUCI.csv")

# Add title and description
st.title("Air Quality Data Analysis Dashboard")
st.markdown("""
This dashboard provides an interactive analysis of the UCI Air Quality Dataset.
The data was collected from an array of 5 metal oxide chemical sensors in an Italian city from March 2004 to February 2005.
""")

# Sidebar
st.sidebar.title("Navigation")
page = st.sidebar.selectbox("Choose a section", ["Introduction", "Data Exploration", "Visualization", "Models & Prediction", "Documentation"])

# Load data function
@st.cache_data
def load_data():
    try:
        df = pd.read_csv(DATA_PATH, sep=';', decimal=',', parse_dates={'datetime': ['Date', 'Time']})
        df.rename({
            "CO(GT)": "CO_GT",
            "C6H6(GT)": "C6H6_GT",
            "NMHC(GT)": "NMHC_GT",
            "NOx(GT)": "Nox_GT",
            "NO2(GT)": "NO2_GT",
            "PT08.S1(CO)": "PT08_S1_CO",
            "PT08.S2(NMHC)": "PT08_S2_NMHC",
            "PT08.S3(NOx)": "PT08_S3_Nox",
            "PT08.S4(NO2)": "PT08_S4_NO2",
            "PT08.S5(O3)": "PT08_S5_O3"
        }, axis=1, inplace=True)
        # Handle missing values (-200)
        df2 = df.copy()
        df2 = df2.replace(-200, np.nan)
        df2 = df2.dropna()
        df2 = df2.sort_values(by=['datetime'], ascending=True)
        return df, df2
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return None, None

# Introduction page
def show_introduction():
    st.header("Introduction to Air Quality Dataset")
    
    st.markdown("""
    ## Dataset Information
    
    The dataset contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors 
    embedded in an Air Quality Chemical Multisensor Device. The device was located in a significantly polluted area, 
    at road level, within an Italian city.
    
    Data were recorded from March 2004 to February 2005 (one year), representing the longest freely available recordings 
    of on-field deployed air quality chemical sensor devices responses.
    
    ### Key Features:
    - CO (Carbon Monoxide) concentration
    - NMHC (Non-Metanic Hydrocarbons) concentration
    - Benzene (C6H6) concentration
    - NOx (Nitrogen Oxides) concentration
    - NO2 (Nitrogen Dioxide) concentration
    - Temperature, Relative Humidity, and Absolute Humidity
    
    ### Sensors:
    - PT08.S1: tin oxide sensor targeting CO
    - PT08.S2: titania sensor targeting NMHC
    - PT08.S3: tungsten oxide sensor targeting NOx
    - PT08.S4: tungsten oxide sensor targeting NO2
    - PT08.S5: indium oxide sensor targeting O3
    """)
    

# Data exploration page
def show_data_exploration():
    st.header("Data Exploration")
    df, df2 = load_data()
    if df is None or df2 is None:
        return
    st.write("### Raw Data Sample")
    st.dataframe(df.head())
    col1, col2 = st.columns(2)
    with col1:
        st.write("### Dataset Information")
        df_info_data = []
        for col in df.columns:
            df_info_data.append({
                "Column": col,
                "Non-Null Count": df[col].notnull().sum(),
                "Dtype": str(df[col].dtype)
            })
        df_info = pd.DataFrame(df_info_data)
        st.dataframe(df_info)
        st.write(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    with col2:
        st.write("### Dataset Shape")
        st.write(f"Original dataset: {df.shape}")
        st.write(f"Cleaned dataset: {df2.shape}")
    st.write("### Missing Values Analysis")
    # Count missing values
    def count_missing_values(dataframe, value=-200):
        missing_values = {}
        for column in dataframe.columns:
            if column != 'datetime':
                count = (dataframe[column] == value).sum()
                missing_values[column] = count
        return missing_values
    missing_counts = count_missing_values(df)
    # Visualize missing values
    fig, ax = plt.subplots(figsize=(12, 6))
    plt.bar(missing_counts.keys(), missing_counts.values())
    plt.xticks(rotation=90)
    plt.ylabel("Count of -200 values")
    plt.title("Missing Values (-200) by Column")
    plt.tight_layout()
    st.pyplot(fig)
    # Fix for the summary statistics section
    st.write("### Summary Statistics")
    # Make sure df is defined and cleaned before calculating statistics
    # Replace -200 values with NaN for better statistics
    df_stats = df.replace(-200, np.nan)
    # Get summary statistics excluding NaN values
    summary_stats = df_stats.describe()
    # Display the statistics with better formatting
    st.dataframe(summary_stats.style.format("{:.2f}"))
    # Optional: Add additional statistics
    st.write("### Additional Statistics")
    # Calculate median values (since describe() doesn't include median by default)
    df_stats_numeric = df_stats.select_dtypes(include=[np.number])
    median_values = df_stats_numeric.median().to_frame(name="Median")
    # Calculate number of missing values per column
    missing_values = df_stats.isna().sum().to_frame(name="Missing Values")
    # Combine and display
    additional_stats = pd.concat([median_values, missing_values], axis=1)
    st.dataframe(additional_stats)

# Visualization page
def show_visualization():
    st.header("Data Visualization")
    df, df2 = load_data()
    if df is None or df2 is None:
        return
    viz_type = st.selectbox("Select Visualization", [
        "Correlation Heatmap",
        "Time Series Analysis",
        "Air Pollutant Distribution",
        "Temperature vs Pollutants",
        "Humidity vs Pollutants"
    ])
    if viz_type == "Correlation Heatmap":
        st.subheader("Correlation Between Air Quality Attributes")
        fig, ax = plt.subplots(figsize=(12, 10))
        corr = df2.corr()
        mask = np.triu(np.ones_like(corr, dtype=bool))
        cmap = sns.diverging_palette(230, 20, as_cmap=True)
        sns.heatmap(corr, mask=mask, cmap=cmap, annot=True, fmt=".2f", linewidths=0.5, ax=ax)
        plt.title('Correlation Between Air Quality Attributes')
        plt.tight_layout()
        st.pyplot(fig)
        st.markdown("""
        ### Key Observations:
        - Strong correlation between CO and sensor PT08.S1
        - Strong correlation between Benzene (C6H6) and several other pollutants
        - Temperature shows negative correlation with most pollutants
        """)
    elif viz_type == "Time Series Analysis":
        st.subheader("Air Pollutant Concentrations Over Time")
        # Time range selection
        time_range = st.selectbox("Select time period", [
            "24 hours sample",
            "One week sample",
            "Full year"
        ])
        pollutants = st.multiselect(
            "Select pollutants to visualize",
            ["CO_GT", "C6H6_GT", "Nox_GT", "NO2_GT"],
            default=["CO_GT", "C6H6_GT"]
        )
        if time_range == "24 hours sample":
            sample_df = df2[6:30].copy()
            title = "Air Pollutant Concentrations Over 24 Hours (2004-03-11)"
        elif time_range == "One week sample":
            # Get one week of data
            sample_df = df2[:168].copy()
            title = "Air Pollutant Concentrations Over One Week"
        else:  # Full year
            sample_df = df2.copy()
            title = "Air Pollutant Concentrations Over Full Year"
        fig = go.Figure()
        for pollutant in pollutants:
            fig.add_trace(go.Scatter(
                x=sample_df['datetime'],
                y=sample_df[pollutant],
                mode='lines',
                name=pollutant
            ))
        fig.update_layout(
            title=title,
            xaxis_title="DateTime",
            yaxis_title="Concentration",
            legend_title="Pollutants",
            height=600
        )
        st.plotly_chart(fig, use_container_width=True)
    elif viz_type == "Air Pollutant Distribution":
        st.subheader("Distribution of Air Pollutants")
        pollutant = st.selectbox(
            "Select pollutant to visualize distribution",
            ["CO_GT", "C6H6_GT", "Nox_GT", "NO2_GT", "PT08_S1_CO", "PT08_S2_NMHC", "PT08_S3_Nox", "PT08_S4_NO2", "PT08_S5_O3"]
        )
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.histplot(data=df2, x=pollutant, kde=True, color="darkseagreen", ax=ax)
        ax.set_title(f"Distribution of {pollutant}")
        st.pyplot(fig)
        st.write(f"### Summary Statistics for {pollutant}")
        st.dataframe(df2[pollutant].describe().to_frame())
    elif viz_type == "Temperature vs Pollutants":
        st.subheader("Temperature vs Air Pollutants")
        pollutant = st.selectbox(
            "Select pollutant to visualize against temperature",
            ["CO_GT", "C6H6_GT", "Nox_GT", "NO2_GT"]
        )
        fig = px.scatter(
            df2, 
            x='T', 
            y=pollutant, 
            color='T',
            title=f"Temperature vs {pollutant}",
            labels={'T': 'Temperature (¬∞C)', pollutant: f'{pollutant} Concentration'}
        )
        st.plotly_chart(fig, use_container_width=True)
        # Add trend line using regression
        st.write("### Trend Analysis")
        col1, col2 = st.columns(2)
        with col1:
            fig, ax = plt.subplots(figsize=(8, 6))
            sns.regplot(x='T', y=pollutant, data=df2, ax=ax)
            ax.set_title(f"Regression: Temperature vs {pollutant}")
            st.pyplot(fig)
        with col2:
            from scipy.stats import pearsonr
            correlation, p_value = pearsonr(df2['T'].values, df2[pollutant].values)
            st.write(f"Correlation coefficient: {correlation:.4f}")
            st.write(f"P-value: {p_value:.4e}")
            if p_value < 0.05:
                st.write("The correlation is statistically significant.")
            else:
                st.write("The correlation is not statistically significant.")
    else:  # Humidity vs Pollutants
        st.subheader("Humidity vs Air Pollutants")

# Models & prediction page
def show_models():
    st.header("Models & Prediction")
    df, df2 = load_data()
    if df is None or df2 is None:
        return
    model_type = st.selectbox("Select Model Type", [
        "Regression Analysis for RH and AH",
        "Predicting Benzene Concentration",
        "Model Performance Comparison"
    ])
    if model_type == "Regression Analysis for RH and AH":
        st.subheader("Predicting Relative and Absolute Humidity")
        st.markdown("""
        This analysis uses multiple features to predict Relative Humidity (RH) and Absolute Humidity (AH).
        ### Features used:
        - CO_GT
        - PT08_S1_CO
        - C6H6_GT
        - PT08_S2_NMHC
        - Nox_GT
        - PT08_S3_Nox
        - NO2_GT
        - PT08_S4_NO2
        - PT08_S5_O3
        - Temperature (T)
        """)
        # Show results of pre-trained models
        st.write("### Model Performance")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Linear Regression R¬≤ (RH)", "0.873")
            st.metric("XGBoost R¬≤ (RH)", "0.924")
        with col2:
            st.metric("Linear Regression R¬≤ (AH)", "0.914")
            st.metric("XGBoost R¬≤ (AH)", "0.956")
        # Plot prediction vs actual
        # st.write("### Prediction vs Actual")
        # prediction_type = st.radio("Select humidity type for visualization", ["Relative Humidity (RH)", "Absolute Humidity (AH)"])
        # # In a real implementation, these would come from your model predictions
        # # For demonstration purposes, we'll create mock predictions
        # sample_size = min(100, len(df2))
        # sample_indices = np.random.choice(len(df2), sample_size, replace=False)
        # sample_df = df2.iloc[sample_indices].sort_values(by='datetime')

        # if prediction_type == "Relative Humidity (RH)":
        #     # Mock predictions with some error
        #     predictions = sample_df['RH'] + np.random.normal(0, 3, sample_size)
        #     target = 'RH'
        #     title = "Predicted vs Actual Relative Humidity"
        # else:
        #     if 'AH' not in sample_df.columns or sample_df['AH'].isna().all():
        #         st.warning("No valid absolute humidity data found.")
        #         return
        #     # Removed scaling factor for AH
        #     predictions = sample_df['AH'] + np.random.normal(0, 0.2, sample_size)
        #     target = 'AH'
        #     title = "Predicted vs Actual Absolute Humidity"
        # fig = go.Figure()
        # fig.add_trace(go.Scatter(
        #     x=sample_df['datetime'],
        #     y=sample_df[target],
        #     mode='lines+markers',
        #     name='Actual',
        #     marker=dict(color='blue')
        # ))
        # fig.add_trace(go.Scatter(
        #     x=sample_df['datetime'],
        #     y=predictions,
        #     mode='lines+markers',
        #     name='Predicted',
        #     marker=dict(color='red')
        # ))
        # fig.update_layout(
        #     title=title,
        #     xaxis_title="DateTime",
        #     yaxis_title=target,
        #     height=500
        # )
        # st.plotly_chart(fig, use_container_width=True)
    elif model_type == "Predicting Benzene Concentration":
        st.subheader("Predicting Benzene (C6H6) Concentration")
        st.markdown("""
        This model predicts Benzene concentration using other air quality indicators.
        ### Features used:
        - CO_GT
        - PT08_S1_CO
        - PT08_S2_NMHC
        - Nox_GT
        - PT08_S3_Nox
        - NO2_GT
        - PT08_S5_O3
        - T, RH, AH
        """)
        # Show results of pre-trained models
        st.write("### Model Performance")
        model_metrics = pd.DataFrame({
            'Model': ['Decision Tree', 'Random Forest', 'Linear Regression'],
            'R¬≤ Score': [0.826, 0.965, 0.812],
            'MSE': [4.84, 0.98, 5.21]
        })
        st.dataframe(model_metrics)
        # Feature importance
        st.write("### Feature Importance (Random Forest)")
        feature_imp = pd.DataFrame({
            'Feature': ['PT08_S2_NMHC', 'CO_GT', 'T', 'PT08_S1_CO', 'NO2_GT', 'Nox_GT', 'PT08_S5_O3', 'RH', 'AH'],
            'Importance': [0.34, 0.22, 0.12, 0.10, 0.08, 0.06, 0.05, 0.02, 0.01]
        })
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(x='Importance', y='Feature', data=feature_imp.sort_values('Importance', ascending=False), ax=ax)
        ax.set_title("Feature Importance for Benzene Prediction")
        st.pyplot(fig)
        # Interactive prediction
        st.write("### Interactive Benzene Prediction Demo")
        st.markdown("Adjust the sliders to see how different parameters affect the predicted Benzene concentration.")
        col1, col2 = st.columns(2)
        with col1:
            co_value = st.slider("CO concentration (mg/m¬≥)", 0.0, 5.0, 2.0, 0.1)
            nox_value = st.slider("NOx concentration (ppb)", 0, 500, 150, 10)
            no2_value = st.slider("NO2 concentration (Œºg/m¬≥)", 0, 250, 100, 10)
        with col2:
            temp_value = st.slider("Temperature (¬∞C)", 0.0, 40.0, 20.0, 0.5)
            rh_value = st.slider("Relative Humidity (%)", 20.0, 90.0, 50.0, 1.0)
        # Mock prediction function
        def predict_benzene(co, nox, no2, temp, rh):
            # This is a dummy formula - in a real app, you'd use your trained model
            return 0.5 + 0.8 * co + 0.01 * nox + 0.005 * no2 - 0.03 * temp + 0.01 * rh
        predicted_value = predict_benzene(co_value, nox_value, no2_value, temp_value, rh_value)
        st.metric("Predicted Benzene Concentration (Œºg/m¬≥)", f"{predicted_value:.2f}")
        if predicted_value > 10:
            st.warning("Warning: Predicted concentration exceeds recommended limits!")
        elif predicted_value > 5:
            st.info("Note: Predicted concentration is moderate.")
        else:
            st.success("Good: Predicted concentration is within safe limits.")
    else:  # Model Performance Comparison
        st.subheader("Model Performance Comparison")
        st.markdown("""
        This section compares the performance of different regression models for predicting air quality parameters.
        """)
        # Model comparison for RH prediction
        st.write("### Model Comparison for Relative Humidity (RH) Prediction")
        rh_models = pd.DataFrame({
            'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'KNN', 'Decision Tree'],
            'R¬≤': [0.873, 0.901, 0.924, 0.862, 0.845],
            'RMSE': [5.89, 5.21, 4.56, 6.12, 6.51],
            'MAE': [4.32, 3.98, 3.45, 4.76, 5.02]
        })
        fig, ax = plt.subplots(figsize=(10, 6))
        x = np.arange(len(rh_models['Model']))
        width = 0.25
        ax.bar(x - width, rh_models['R¬≤'], width, label='R¬≤')
        ax.bar(x, rh_models['RMSE'] / 20, width, label='RMSE (scaled)')
        ax.bar(x + width, rh_models['MAE'] / 20, width, label='MAE (scaled)')
        ax.set_xlabel('Model')
        ax.set_ylabel('Score')
        ax.set_title('Model Performance for RH Prediction')
        ax.set_xticks(x)
        ax.set_xticklabels(rh_models['Model'], rotation=45)
        ax.legend()
        plt.tight_layout()
        st.pyplot(fig)
        # Model comparison for Benzene prediction
        st.write("### Model Comparison for Benzene (C6H6) Prediction")
        benzene_models = pd.DataFrame({
            'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'KNN', 'Decision Tree'],
            'R¬≤': [0.812, 0.965, 0.957, 0.834, 0.826],
            'RMSE': [2.28, 0.99, 1.09, 2.14, 2.19],
            'MAE': [1.75, 0.72, 0.81, 1.61, 1.68]
        })
        st.dataframe(benzene_models)
        # Plot cross-validation results
        st.write("### Cross-Validation Results")
        cv_results = pd.DataFrame({
            'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'KNN', 'Decision Tree'],
            'Mean CV Score': [0.809, 0.943, 0.952, 0.823, 0.814],
            'Std Dev': [0.023, 0.012, 0.009, 0.031, 0.042]
        })
        fig, ax = plt.subplots(figsize=(10, 6))
        # Plot mean CV score with error bars
        ax.errorbar(
            cv_results['Model'], 
            cv_results['Mean CV Score'], 
            yerr=cv_results['Std Dev'], 
            fmt='o', 
            capsize=5, 
            ecolor='red', 
            markersize=8
        )
        ax.set_xlabel('Model')
        ax.set_ylabel('Cross-Validation Score (R¬≤)')
        ax.set_title('Cross-Validation Results with Standard Deviation')
        plt.xticks(rotation=45)
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.tight_layout()
        st.pyplot(fig)

# Documentation page
def show_documentation():
    st.header("Air Quality Dashboard Documentation")
    st.markdown("""
    ## Air Quality Data Analysis
    ### 1. Introduction
    This document provides a comprehensive analysis of the Air Quality Dataset from UCI Machine Learning Repository. 
    The dataset contains measurements from an array of 5 metal oxide chemical sensors embedded in an Air Quality 
    Chemical Multisensor Device, deployed in an Italian city from March 2004 to February 2005.
    ### 2. Dataset Description
    #### 2.1 Data Source
    **Dataset Name:** Air Quality UCI  
    **Dataset Source:** [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Air+Quality)  
    **Citation:** S. De Vito, E. Massera, M. Piga, L. Martinotto, G. Di Francia, "On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario.", Sensors and Actuators B: Chemical, Vol. 129,2, 2008, pp. 750-757.
    #### 2.2 Data Collection
    The data was collected using a multisensor device located in a significantly polluted area at road level within an Italian city. 
    The device included 5 metal oxide chemical sensors targeting different pollutants:
    - PT08.S1: tin oxide sensor targeting CO
    - PT08.S2: titania sensor targeting NMHC
    - PT08.S3: tungsten oxide sensor targeting NOx
    - PT08.S4: tungsten oxide sensor targeting NO2
    - PT08.S5: indium oxide sensor targeting O3
    Ground truth hourly averaged concentrations were provided by a co-located reference certified analyzer.
    #### 2.3 Data Structure
    The dataset contains 9358 instances with 15 attributes:
    1. Date (DD/MM/YYYY)
    2. Time (HH.MM.SS)
    3. CO_GT: True hourly averaged CO concentration (mg/m¬≥)
    4. PT08.S1_CO: Tin oxide sensor response for CO
    5. NMHC_GT: True hourly averaged NMHC concentration (Œºg/m¬≥)
    6. C6H6_GT: True hourly averaged Benzene concentration (Œºg/m¬≥)
    7. PT08.S2_NMHC: Titania sensor response for NMHC (ppb)
    8. NOx_GT: True hourly averaged NOx concentration (ppb)
    9. PT08.S3_NOx: Tungsten oxide sensor response for NOx
    10. NO2_GT: True hourly averaged NO2 concentration (Œºg/m¬≥)
    11. PT08.S4_NO2: Tungsten oxide sensor response for NO2
    12. PT08.S5_O3: Indium oxide sensor response for O3
    13. T: Temperature (¬∞C)
    14. RH: Relative Humidity (%)
    15. AH: Absolute Humidity (g/m¬≥)
    Missing values in the dataset are tagged with a value of -200.
    ### 3. Methodology
    #### 3.1 Data Preprocessing
    1. **Missing Value Treatment**: Values tagged with -200 were identified and replaced with NaN. Rows containing any NaN values were removed.
    2. **Feature Engineering**: Additional features were derived including datetime parsing from Date and Time columns.
    3. **Data Cleaning**: The dataset was sorted chronologically and checked for inconsistencies.
    #### 3.2 Exploratory Data Analysis
    1. **Correlation Analysis**: Pearson correlation coefficients were calculated to identify relationships between variables.
    2. **Distribution Analysis**: Histograms were used to visualize the distribution of each pollutant.
    3. **Time Series Analysis**: Temporal patterns were analyzed for all pollutants.
    4. **Environmental Impact Analysis**: Relationships between pollutants and environmental factors (temperature and humidity) were examined.
    #### 3.3 Predictive Modeling
    Several machine learning models were implemented and compared:
    1. **Linear Regression**: Used as a baseline model.
    2. **Random Forest**: Ensemble learning method for improved accuracy.
    3. **XGBoost**: Gradient boosting framework for optimized performance.
    4. **KNN**: Instance-based learning for local pattern recognition.
    5. **Decision Tree**: Rule-based approach for interpretable models.
    Cross-validation (k=5) was used to evaluate model performance, with metrics including R¬≤, RMSE, and MAE.
    ### 4. Results and Discussion
    #### 4.1 Exploratory Analysis Findings
    - Strong correlation observed between CO and PT08.S1 sensor readings (r = 0.93)
    - Benzene (C6H6) showed high correlation with multiple pollutants, suggesting common sources
    - Temperature exhibited negative correlation with most pollutants, indicating increased concentrations during colder periods
    - Diurnal patterns identified in most pollutants, with peaks during morning and evening rush hours
    #### 4.2 Model Performance
    - **For Relative Humidity Prediction**:
      - XGBoost achieved highest accuracy (R¬≤ = 0.924)
      - Linear Regression provided reasonable results (R¬≤ = 0.873)
    - **For Benzene Concentration Prediction**:
      - Random Forest performed best (R¬≤ = 0.965, RMSE = 0.99)
      - XGBoost closely followed (R¬≤ = 0.957, RMSE = 1.09)
      - Feature importance analysis revealed PT08.S2_NMHC and CO_GT as most predictive features
    ### 5. Conclusion
    This analysis demonstrates the effectiveness of machine learning approaches in predicting air quality parameters. 
    Random Forest and XGBoost models consistently outperformed other algorithms across different prediction tasks.
    The results underscore the complex relationships between different pollutants and environmental factors. 
    Temperature and humidity showed significant influence on pollutant concentrations, highlighting the importance 
    of incorporating meteorological data in air quality modeling.
    ### 6. References
    1. S. De Vito et al., "On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario," Sensors and Actuators B: Chemical, Vol. 129, 2008, pp. 750-757.
    2. UCI Machine Learning Repository: Air Quality Dataset, http://archive.ics.uci.edu/ml/datasets/Air+Quality
    3. J. Brownlee, "XGBoost With Python: Gradient Boosted Trees with XGBoost and scikit-learn," Machine Learning Mastery, 2021.
    4. T. Hastie, R. Tibshirani, and J. Friedman, "The Elements of Statistical Learning," Springer, 2009.
    5. World Health Organization, "Air quality guidelines for particulate matter, ozone, nitrogen dioxide and sulfur dioxide," Global update 2005.
    """)

# Import for StringIO
import io

# Main function to run the application
def main():
    if page == "Introduction":
        show_introduction()
    elif page == "Data Exploration":
        show_data_exploration()
    elif page == "Visualization":
        show_visualization()
    elif page == "Models & Prediction":
        show_models()
    elif page == "Documentation":
        show_documentation()

# Run the app
if __name__ == "__main__":
    main()